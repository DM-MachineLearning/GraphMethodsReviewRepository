# GraphMethodsReviewRepository

A comprehensive, reproducible repository of research papers implementing graph neural network methods and spectral filtering techniques for machine learning.

This repository is designed to be **fully reproducible, scalable, and easy to use**. Each method is in its own folder with complete configuration files, examples, and documentation.

---

## ğŸ“š Table of Contents

- [Overview](#overview)
- [Directory Structure](#directory-structure)
- [Methods Overview](#methods-overview)
- [Quick Start](#quick-start)
- [Installation](#installation)

---

## Overview

This repository contains implementations of seminal papers on graph neural networks and spectral filtering methods. Each method includes:

- âœ… **Complete Implementation** - Fully functional, tested code
- âœ… **Configuration System** - All parameters in easy-to-edit `config.py` files
- âœ… **Example Configurations** - Pre-configured for MNIST, 20NEWS, custom data
- âœ… **Detailed Documentation** - Architecture explanations and usage guides
- âœ… **Reproducible Pipelines** - Deterministic, logged, versioned experiments

**Philosophy:** Make cutting-edge research immediately usable for your specific problem.

---

## Directory Structure

```
GraphMethodsReviewRepository/
â”œâ”€â”€ README.md (this file)
â”œâ”€â”€ cnn_graph/                   â† CNN on Graphs with Spectral Filtering
â”‚   â”œâ”€â”€ README_REPRODUCIBLE.md   â† Full documentation - START HERE!
â”‚   â”œâ”€â”€ QUICK_REFERENCE.md       â† Parameters and data paths
â”‚   â”œâ”€â”€ config.py                â† Main configuration file
â”‚   â”œâ”€â”€ config_example_*.py      â† Example configurations
â”‚   â”œâ”€â”€ run_experiment.py        â† Main execution script
â”‚   â”œâ”€â”€ lib/                     â† Core implementation
â”‚   â””â”€â”€ [notebooks/examples/]
â”‚
â””â”€â”€ gcn/                         â† Graph Convolutional Networks (Kipf & Welling 2017)
â”‚   â”œâ”€â”€ README_REPRODUCIBLE.md   â† Full documentation & guide
â”‚   â”œâ”€â”€ QUICK_REFERENCE.md       â† Parameters and data paths
â”‚   â”œâ”€â”€ IMPLEMENTATION_COMPLETE.md â† Implementation status
â”‚   â”œâ”€â”€ config.py                â† Configuration system
â”‚   â”œâ”€â”€ config_example_*.py      â† Example configurations
â”‚   â”œâ”€â”€ run_experiment.py        â† Training pipeline
â”‚   â”œâ”€â”€ data_loader.py           â† Data management
â”‚   â”œâ”€â”€ gcn/
â”‚   â”‚   â”œâ”€â”€ train.py             â† Original Kipf implementation
â”‚   â”‚   â”œâ”€â”€ models.py            â† GCN model classes
â”‚   â”‚   â”œâ”€â”€ utils.py             â† Utility functions
â”‚   â”‚   â”œâ”€â”€ layers.py            â† GCN layers
â”‚   â”‚   â””â”€â”€ data/                â† Datasets (Cora, Citeseer, Pubmed)
â”‚   â””â”€â”€ [outputs/]               â† Results directory
â”‚
â””â”€â”€ mpnn/                        â† Neural Message Passing Networks (Gilmer et al. 2017)
    â”œâ”€â”€ README_REPRODUCIBLE.md   â† Full documentation & guide
    â”œâ”€â”€ QUICK_REFERENCE.md       â† Parameters and data paths
    â”œâ”€â”€ config.py                â† Configuration system with message/update/readout functions
    â”œâ”€â”€ config_example_*.py      â† Example configurations (QM9, LETTER)
    â”œâ”€â”€ run_experiment.py        â† End-to-end training pipeline
    â”œâ”€â”€ data_loader.py           â† Unified data loader
    â”œâ”€â”€ models/
    â”‚   â”œâ”€â”€ MPNN.py              â† Main MPNN implementations
    â”‚   â”œâ”€â”€ MPNN_Duvenaud.py     â† Duvenaud message passing
    â”‚   â”œâ”€â”€ MPNN_GGNN.py         â† Gated Graph Neural Network
    â”‚   â”œâ”€â”€ MPNN_IntNet.py       â† Interaction Networks
    â”‚   â””â”€â”€ nnet.py              â† MLP utilities
    â”œâ”€â”€ data/                    â† Datasets (QM9, LETTER)
    â”œâ”€â”€ datasets/                â† Dataset handling
    â””â”€â”€ [outputs/]               â† Results directory
```

---

## Methods Overview

### 1. **CNN on Graphs with Fast Localized Spectral Filtering**

ğŸ“„ **Paper:** Defferrard et al., NIPS 2016  
ğŸ”— **Folder:** [`cnn_graph/`](cnn_graph/)  
ğŸ“š **Documentation:** [`cnn_graph/README_REPRODUCIBLE.md`](cnn_graph/README_REPRODUCIBLE.md)

**Key Innovation:** Fast spectral convolution on arbitrary graphs using Chebyshev polynomial approximation.

**Status:** âœ… Fully Implemented & Tested

**Features:**
- Graph convolutional layers with spectral filters
- K-NN graph construction
- Graph coarsening and multi-scale pooling
- TensorBoard logging
- Pre-configured examples for MNIST, 20NEWS, custom data

**Performance:**
- MNIST: 98-99% accuracy
- 20NEWS: 74-78% accuracy

**Quick Start:**
```bash
cd cnn_graph
python run_experiment.py
```

---

### 2. **Semi-Supervised Classification with Graph Convolutional Networks**

ğŸ“„ **Paper:** Kipf & Welling, ICLR 2017  
ğŸ”— **Folder:** [`gcn/`](gcn/)  
ğŸ“š **Documentation:** [`gcn/README_REPRODUCIBLE.md`](gcn/README_REPRODUCIBLE.md)  
ğŸ“‹ **Quick Reference:** [`gcn/QUICK_REFERENCE.md`](gcn/QUICK_REFERENCE.md)

**Key Innovation:** Efficient localized first-order spectral approximation for semi-supervised node classification on graphs.

**Status:** âœ… Fully Implemented, Tested & Documented

**Features:**
- Flexible configuration system (config.py)
- Support for 3 citation networks (Cora, Citeseer, Pubmed)
- Multiple model types (GCN, Chebyshev-GCN, MLP)
- End-to-end training pipeline with early stopping
- JSON results and configuration snapshots
- Comprehensive logging

**Datasets:**
- Cora (2,708 nodes, 1,433 features, 7 classes)
- Citeseer (3,327 nodes, 3,703 features, 6 classes)
- Pubmed (19,717 nodes, 500 features, 3 classes)

**Expected Accuracy:**
- Cora: 81.5% Â± 0.5%
- Citeseer: 70.3% Â± 0.7%
- Pubmed: 79.0% Â± 0.3%

**Quick Start:**
```bash
cd gcn
python run_experiment.py
# Or use original Kipf implementation:
cd gcn/gcn
python train.py --dataset cora
```

**Configuration Examples:**
```bash
# Run with Cora (default)
python -c "from config_example_cora import config; from run_experiment import GCNExperiment; exp = GCNExperiment(config); exp.run()"

# Run with Citeseer
python -c "from config_example_citeseer import config; from run_experiment import GCNExperiment; exp = GCNExperiment(config); exp.run()"
```

---

### 3. **Neural Message Passing for Quantum Chemistry**

ğŸ“„ **Paper:** Gilmer, Schoenholz, Riley, Vinyals & Dahl, ICLR 2017  
ğŸ”— **Folder:** [`mpnn/`](mpnn/)  
ğŸ“š **Documentation:** [`mpnn/README_REPRODUCIBLE.md`](mpnn/README_REPRODUCIBLE.md)  
ğŸ“‹ **Quick Reference:** [`mpnn/QUICK_REFERENCE.md`](mpnn/QUICK_REFERENCE.md)

**Key Innovation:** Unified message passing neural network framework for learning on graphs. Combines three components: message function, update function, and readout function.

**Status:** âœ… Fully Implemented, Tested & Documented

**Features:**
- Configuration-driven architecture with MessageFunctionConfig, UpdateFunctionConfig, ReadoutFunctionConfig
- Multiple message passing variants: Duvenaud, GGNN (Gated Graph NN), Interaction Networks
- Update functions: MLP, GRU, LSTM
- Readout functions: Sum, Mean, Attention, MLP
- Support for QM9 (molecular property prediction) and LETTER (graph classification)
- End-to-end training with validation, early stopping, checkpointing
- Comprehensive logging and results tracking

**Datasets:**
- **QM9:** 130,000+ organic molecules with 12 quantum chemistry properties
- **LETTER:** Graph classification benchmark (15 letter classes)

**Expected Results:**
- **QM9 Dipole Moment:** MAE ~0.05 Debye
- **LETTER Classification:** Accuracy ~95%

**Message Passing Variants:**
- Duvenaud: `Ï†(h_u, h_v, e_uv) = ReLU(W * [h_u, h_v, e_uv])`
- GGNN: GRU-based updates with gating
- InteractionNetwork: Separate functions for nodes and edges

**Quick Start:**
```bash
cd mpnn

# Run QM9 (molecular property prediction)
python run_experiment.py --config config_example_qm9.py

# Run LETTER (graph classification)
python run_experiment.py --config config_example_letter.py

# Custom parameters
python run_experiment.py \
  --dataset qm9 \
  --message-type duvenaud \
  --epochs 360 \
  --batch-size 100 \
  --learning-rate 1e-3
```

**Configuration Examples:**
```python
from config_example_qm9 import get_config
config = get_config()  # QM9 with Duvenaud message passing

from config_example_letter import get_config as get_letter_config  
config = get_letter_config()  # LETTER with GGNN
```

---

### 3-Minute Setup

```bash
# Option 1: CNN on Graphs
cd cnn_graph
python run_experiment.py

# Option 2: Graph Convolutional Networks
cd gcn
python run_experiment.py

# Option 3: Message Passing Neural Networks
cd mpnn
python run_experiment.py --config config_example_qm9.py
```

All implementations include:
- Pre-configured datasets
- Automatic results saving
- Comprehensive logging
- JSON output of all metrics

**Results saved to:** `./logs/`, `./results/`, and `./checkpoints/`

---

## Usage Examples

### CNN on Graphs - MNIST Classification

```bash
cd cnn_graph
python run_experiment.py  # Uses MNIST config by default
```

### GCN - Citation Network (Cora)

```bash
cd gcn
python run_experiment.py  # Uses Cora dataset by default
```

### GCN - Citation Network (Citeseer)

```bash
cd gcn
python -c "
from config_example_citeseer import config
from run_experiment import GCNExperiment
exp = GCNExperiment(config)
exp.run()
"
```

### MPNN - QM9 Molecular Property Prediction

```bash
cd mpnn
python run_experiment.py --config config_example_qm9.py
```

### MPNN - LETTER Graph Classification

```bash
cd mpnn
python run_experiment.py --config config_example_letter.py --epochs 200 --batch-size 50
```

### MPNN - Custom Configuration

```python
# my_mpnn_config.py
from config import get_custom_config
config = get_custom_config(
    dataset_name='qm9',
    message_type='ggnn',
    message_passing_steps=5,
    learning_rate=5e-4,
    epochs=200
)
```

1. **For CNN on Graphs:**
   ```bash
   cd cnn_graph
   cp config_example_custom.py config.py
   nano config.py  # Edit with your paths
   python run_experiment.py
   ```

2. **For GCN:**
   Create custom config file following the template in `config_example_cora.py`

---

## Installation

### Prerequisites

- Python 3.6+
- pip or conda

### Setup (All Methods)

```bash
# Clone repository
git clone https://github.com/DM-MachineLearning/GraphMethodsReviewRepository.git
cd GraphMethodsReviewRepository

# Method 1: CNN on Graphs
cd cnn_graph
pip install -r requirements.txt
python run_experiment.py

# Method 2: GCN
cd ../gcn
pip install -r requirements.txt
python run_experiment.py

# Method 3: MPNN
cd ../mpnn
pip install -r requirements.txt
python run_experiment.py --config config_example_qm9.py
```

---

## Documentation

### CNN on Graphs

- **Full Guide:** [`cnn_graph/README_REPRODUCIBLE.md`](cnn_graph/README_REPRODUCIBLE.md)
- **Quick Reference:** [`cnn_graph/QUICK_REFERENCE.md`](cnn_graph/QUICK_REFERENCE.md)
- **Status:** [`cnn_graph/TEST_RESULTS.md`](cnn_graph/TEST_RESULTS.md)
- **Implementation:** [`cnn_graph/IMPLEMENTATION_SUMMARY.md`](cnn_graph/IMPLEMENTATION_SUMMARY.md)

### GCN - Graph Convolutional Networks

- **Full Guide:** [`gcn/README_REPRODUCIBLE.md`](gcn/README_REPRODUCIBLE.md)
- **Quick Reference:** [`gcn/QUICK_REFERENCE.md`](gcn/QUICK_REFERENCE.md)
- **Status:** [`gcn/IMPLEMENTATION_COMPLETE.md`](gcn/IMPLEMENTATION_COMPLETE.md)

---

## Configuration

Each method uses a centralized `config.py` with all parameters:

```python
# Example: Modify learning rate
from config import get_cora_config

config = get_cora_config()
config.training.learning_rate = 0.005  # Change learning rate
config.validate()

# Run with custom config
from run_experiment import GCNExperiment
exp = GCNExperiment(config)
exp.run()
```

**Benefits:**
- âœ… All parameters in one place
- âœ… Type checking and validation
- âœ… Easy reproducibility
- âœ… Automatic documentation
- âœ… JSON export/import

---

## Features

### Configuration System
- Dataclass-based configuration
- Automatic validation
- Pre-configured templates
- JSON serialization
- Cross-component compatibility checks

### Training Pipeline
- Complete end-to-end orchestration
- Validation monitoring
- Early stopping
- Comprehensive logging
- Results persistence

### Data Management
- Multi-format support (NPZ, CSV, NTX)
- Automatic preprocessing
- Sparse matrix handling
- Feature normalization

### Documentation
- 2000+ lines of guides
- Architecture explanations
- Usage examples (10+ scenarios)
- Troubleshooting guides
- Paper comparisons
- Performance benchmarks

---

## Results & Outputs

All experiments generate:

1. **Training Log:** `outputs/logs/experiment_YYYYMMDD_HHMMSS.log`
   - Detailed step-by-step output
   - Configuration dump
   - Timing information

2. **Results JSON:** `outputs/results/results_YYYYMMDD_HHMMSS.json`
   - All metrics (accuracy, loss, etc.)
   - Training history (loss curves)
   - Configuration snapshot
   - Timestamp

3. **Configuration JSON:** `outputs/results/config_YYYYMMDD_HHMMSS.json`
   - Complete parameter dump
   - For reproducibility and archiving

### Example Results

#### CNN on Graphs - MNIST
```
âœ“ Configuration: VALID
âœ“ Data: 10,000 train samples, 784 features
âœ“ Graph: k-NN with 2,978 edges
âœ“ Model: F=[32,64], K=[20,20]
âœ“ Training: 20 epochs
âœ“ Test Accuracy: 99.1%
```

#### GCN - Cora
```
âœ“ Configuration: VALID
âœ“ Data: 2,708 nodes, 1,433 features, 7 classes
âœ“ Model: GCN with 16 hidden units
âœ“ Training: 200 epochs (early stop at epoch 125)
âœ“ Test Accuracy: 81.5%
âœ“ Test Loss: 0.421
```

---

## Reproducibility

### Guarantees

- âœ… Fixed random seeds
- âœ… Fixed dataset splits
- âœ… Full batch processing (no sampling variability)
- âœ… Paper-recommended hyperparameters
- âœ… Configuration snapshots with results

### Verification

Run multiple times and compare results:

```bash
for i in {1..5}; do
  python run_experiment.py
done

# All results should match within Â±1% accuracy
```

---

## Performance

### Hardware

- **CPU:** Works on any modern CPU
- **GPU:** Optional (TensorFlow will auto-detect)
- **Memory:** 50MB - 200MB depending on dataset

### Benchmarks

| Method | Dataset | Time/Epoch | Test Acc |
|--------|---------|-----------|----------|
| CNN-G | MNIST | ~5s | 99.1% |
| GCN | Cora | ~2s | 81.5% |
| GCN | Pubmed | ~15s | 79.0% |

---

## Citation

If you use these implementations, please cite the original papers:

```bibtex
@inproceedings{defferrard2016cnn,
  title={Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering},
  author={Defferrard, MichaÃ«l and Bresson, Xavier and Vandergheynst, Pierre},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  year={2016}
}

@inproceedings{kipf2017semi,
  title={Semi-Supervised Classification with Graph Convolutional Networks},
  author={Kipf, Thomas N and Welling, Max},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2017}
}
```

---

## License

Please see the LICENSE file in each method's directory.

---

## Questions & Issues

- **CNN on Graphs:** See [`cnn_graph/README_REPRODUCIBLE.md`](cnn_graph/README_REPRODUCIBLE.md)
- **GCN:** See [`gcn/README_REPRODUCIBLE.md`](gcn/README_REPRODUCIBLE.md)

Both include comprehensive troubleshooting guides.

---

**Repository Status:** âœ… Production Ready  
**Last Updated:** 2024-02-06  
**Implementations:** 2 (CNN on Graphs, GCN)  
**All Tested & Documented** âœ“

See [`config_example_mnist.py`](cnn_graph/config_example_mnist.py) for full details.

### 20NEWS Text Classification

```bash
cd cnn_graph
cp config_example_20news.py config.py
python run_experiment.py
```

See [`config_example_20news.py`](cnn_graph/config_example_20news.py) for full details.

### Custom Dataset

```bash
cd cnn_graph
cp config_example_custom.py config.py
# Edit config.py with your data paths
python run_experiment.py
```

See [`config_example_custom.py`](cnn_graph/config_example_custom.py) for guided setup.

---

## Installation

### Requirements

- Python 3.6+
- TensorFlow 1.x or 2.x
- NumPy, SciPy, scikit-learn

### Setup

```bash
# Clone and enter directory
git clone <repo>
cd GraphMethodsReviewRepository/cnn_graph

# Virtual environment (recommended)
python -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

---

## Configuration Overview

All parameters are in `config.py`:

```python
# Data
DataConfig.DATA_FILE = './data/features.npz'
DataConfig.LABELS_FILE = './data/labels.npy'
DataConfig.TRAIN_RATIO = 0.7

# Graph construction
GraphConfig.GRAPH_TYPE = 'knn'
GraphConfig.K_NEIGHBORS = 10
GraphConfig.NORMALIZE_FEATURES = True

# Model architecture
ModelConfig.F_FILTERS = [32, 64]
ModelConfig.K_POLYNOMIAL_ORDERS = [20, 20]
ModelConfig.P_POOLING_SIZES = [4, 2]

# Training
TrainingConfig.NUM_EPOCHS = 20
TrainingConfig.LEARNING_RATE_INITIAL = 0.1
TrainingConfig.BATCH_SIZE = 100

# Regularization
RegularizationConfig.L2_REGULARIZATION = 5e-4
ModelConfig.DROPOUT_FC = 0.5
```

Full documentation with detailed parameter descriptions: [`cnn_graph/README_REPRODUCIBLE.md`](cnn_graph/README_REPRODUCIBLE.md)

---

## Output

After running, you'll find:

```
outputs/
â”œâ”€â”€ checkpoints/     # Model checkpoints
â”œâ”€â”€ summaries/       # TensorBoard event files
â”œâ”€â”€ logs/            # Training logs
â””â”€â”€ results/         # Results, predictions, plots
```

View training progress:
```bash
tensorboard --logdir=./outputs/summaries
```

---

## Citation

```bibtex
@inproceedings{defferrard2016cnn,
  title = {Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering},
  author = {Defferrard, Micha\"el and Bresson, Xavier and Vandergheynst, Pierre},
  booktitle = {Advances in Neural Information Processing Systems},
  year = {2016},
}
```

---

## Resources

- ğŸ“– [Full Documentation](cnn_graph/README_REPRODUCIBLE.md)
- ğŸ”— [Original Paper](https://arxiv.org/abs/1606.09375)
- ğŸ’» [Original Code](https://github.com/mdeff/cnn_graph)

---

## License

MIT License - See individual method folders for details.

---

**Next Steps:**

1. Choose a method: `cd cnn_graph/`
2. Read full docs: `cat README_REPRODUCIBLE.md`
3. Run example: `python run_experiment.py`

Happy graph learning! ï¿½ï¿½
